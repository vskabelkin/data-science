{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "биграммные предложения\n",
      "0 But that's going to be current to generate a large searches correlates what that data collected your body Or the data is search smarter through better data problems up in Python Data is a large collection and analyzed .\n",
      "1 But the technologies the most of the spread of about what the only useful if any CD .\n",
      "2 It was disambiguating Apple job posting you look at LinkedIn .\n",
      "3 But merely using the people and tune the numbers .\n",
      "4 There are then combine the metadata track lengths and its EC2 clusters .\n",
      "5 Amazon understands that knows how to drawing conclusions .\n",
      "6 This is this data trail since many processors and consumers and uses it .\n",
      "7 Whether it's easy to be nice if it necessary to do data scientists particularly physicists rather than for distributing an audio stream processing companies banks and other disciplines it arrives and are easier to make it and Factual provide data .\n",
      "8 There are in racks of known data in a data products .\n",
      "9 If the data we're talking about statistics and added value in size of data might like an end solution to latitude and presenting data were the foreclosures on every track sends it tell its EC2 clusters that can or analyzing an epidemic and more scatter plots trying to define a single tool .\n",
      "\n",
      "триграммные предложения\n",
      "0 While that sounds like a simple task the trick was disambiguating Apple from many job postings in the nascent data industry is trying to build the 2012 Nissan Stanza or Office 2015 they're all trying to find out just how bad your data .\n",
      "1 Whether that data collected from users provides added value .\n",
      "2 The part of the earlier data products .\n",
      "3 That's where services like Amazon's Mechanical Turk come in .\n",
      "4 RAM has moved from 1 000 MB to roughly 25 GB a price reduction of about 40000 to say nothing of the puzzle .\n",
      "5 One of the data was all locked up in proprietary sources and the unique skill sets .\n",
      "6 If you have to look at the data are valid .\n",
      "7 It's easy to turn this into a large difficult problem that gave them the same result .\n",
      "8 If you have a strong mathematical background computing skills and come from a spreadsheet .\n",
      "9 While we aren't drowning in a database of track lengths and then combine the results into a state where it's usable .\n",
      "\n",
      "генерирование предложений на основе грамматики\n",
      "0 big logistic logistic logistic linear logistic regression about big Python near linear data science about linear regression about linear regression near logistic regression near big data science learns\n",
      "1 data science learns linear data science near linear regression\n",
      "2 regression learns\n",
      "3 Python is logistic data science about logistic data science\n",
      "4 linear Python near linear Python is data science\n",
      "5 Python learns Python\n",
      "6 Python trains\n",
      "7 big linear big Python about big data science about logistic Python about logistic regression trains linear data science near logistic regression\n",
      "8 regression trains linear regression about big Python\n",
      "9 big linear data science about linear Python about logistic regression trains\n",
      "\n",
      "сэмплирование по Гиббсу\n",
      "(1, 2) 29 24\n",
      "(4, 7) 32 29\n",
      "(5, 7) 33 29\n",
      "(4, 10) 25 26\n",
      "(5, 6) 29 33\n",
      "(2, 8) 20 24\n",
      "(6, 11) 25 29\n",
      "(6, 9) 26 25\n",
      "(1, 4) 23 39\n",
      "(5, 9) 33 27\n",
      "(3, 9) 26 24\n",
      "(2, 3) 25 27\n",
      "(3, 7) 29 40\n",
      "(6, 7) 28 31\n",
      "(2, 5) 21 34\n",
      "(5, 8) 26 33\n",
      "(4, 6) 29 24\n",
      "(2, 7) 32 28\n",
      "(3, 8) 22 32\n",
      "(5, 10) 27 22\n",
      "(6, 8) 36 24\n",
      "(1, 5) 30 25\n",
      "(6, 10) 22 23\n",
      "(5, 11) 28 26\n",
      "(6, 12) 26 36\n",
      "(2, 6) 34 25\n",
      "(4, 8) 21 31\n",
      "(4, 5) 34 26\n",
      "(3, 5) 36 21\n",
      "(3, 6) 26 19\n",
      "(1, 7) 28 32\n",
      "(1, 3) 33 20\n",
      "(1, 6) 32 27\n",
      "(4, 9) 33 36\n",
      "(3, 4) 19 29\n",
      "(2, 4) 22 20\n",
      "0 Big Data 3\n",
      "0 Java 3\n",
      "0 Hadoop 2\n",
      "0 Spark 1\n",
      "0 Cassandra 1\n",
      "0 C++ 1\n",
      "0 HBase 1\n",
      "0 MapReduce 1\n",
      "0 Storm 1\n",
      "0 deep learning 1\n",
      "0 programming languages 1\n",
      "1 neural networks 2\n",
      "1 HBase 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 MySQL 1\n",
      "1 decision trees 1\n",
      "1 scipy 1\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 artificial intelligence 1\n",
      "1 databases 1\n",
      "1 NoSQL 1\n",
      "1 deep learning 1\n",
      "2 regression 3\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "2 support vector machines 1\n",
      "2 mathematics 1\n",
      "3 probability 3\n",
      "3 statistics 3\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 C++ 1\n",
      "3 theory 1\n",
      "3 artificial intelligence 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "databases 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "databases 2\n",
      "machine learning 2\n",
      "statistics 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "statistics 3\n",
      "machine learning 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Big Data and programming languages 3\n",
      "machine learning 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "statistics 3\n",
      "machine learning 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "databases 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "statistics 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "statistics 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "databases 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "machine learning 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# natural_language_processing.py\n",
    "\n",
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup  # установить pip (pip3) install BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def plot_resumes(plt):\n",
    "    data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]\n",
    "\n",
    "    def text_size(total):\n",
    "        \"\"\"равно 8, если total = 0, и 28, если total = 200\"\"\"\n",
    "        return 8 + total / 200 * 20\n",
    "\n",
    "    for word, job_popularity, resume_popularity in data:\n",
    "        plt.text(job_popularity, resume_popularity, word,\n",
    "                 ha='center', va='center',\n",
    "                 size=text_size(job_popularity + resume_popularity))\n",
    "    plt.xlabel(\"Популярность среди объявлений о вакансиях\")\n",
    "    plt.ylabel(\"Популярность среди резюме\")\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# n-граммные модели языка\n",
    "#\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "def get_document():\n",
    "\n",
    "    url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')   #UPD в оригинале 'html5lib'\n",
    "\n",
    "    content = soup.find(\"div\", \"article-body\")  # найти div с классом entry-content\n",
    "    regex = r\"[\\w']+|[\\.]\"                      # совпадает со словом или точкой\n",
    "\n",
    "    document = []\n",
    "\n",
    "\n",
    "    for paragraph in content(\"p\"):\n",
    "        words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "        document.extend(words)\n",
    "\n",
    "    return document\n",
    "\n",
    "def generate_using_bigrams(transitions):\n",
    "    current = \".\"   # означает, что следующее слово начинает предложение\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # биграммы (current, _)\n",
    "        current = random.choice(next_word_candidates)  # выбрать произвольно\n",
    "        result.append(current)                         # добавить к результатам\n",
    "        if current == \".\": return \" \".join(result)     # если \".\", то закончить\n",
    "\n",
    "def generate_using_trigrams(starts, trigram_transitions):\n",
    "    current = random.choice(starts)   # выбрать произвольное исхолное слово\n",
    "    prev = \".\"                        # и поставить перед ним '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "\n",
    "#\n",
    "# модель языка на основе грамматик\n",
    "#\n",
    "\n",
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # пропустить терминалы\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # choose a replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # если мы тут, значит, нашли нетерминальную лексему,\n",
    "    # произвольно выбрать для нее подстановку\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])\n",
    "\n",
    "#\n",
    "# метод сэмплирования по Гиббсу\n",
    "#\n",
    "\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"равновероятное значение будет x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # если сумма <= 7, первый кубик равновероятно будет равен\n",
    "        # 1, 2, ..., (сумма - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # если сумма > 7, первый кубик равновероятно будет равен\n",
    "        # (сумма - 6), (сумма - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # на самом деле не имеет значения, какие числа\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts\n",
    "\n",
    "#\n",
    "# ТЕМАТИЧЕСКОЕ МОДЕЛИРОВАНИЕ\n",
    "#\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # равномерно между 0 и суммой\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # вернуть наименьший i, такой что\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4\n",
    "\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"доля слов в документе _d_, которые\n",
    "    назначаются тематике _topic_ (плюс некоторое сглаживание)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"доля слов, назначаемых тематике _topic_, которые\n",
    "    равны _word_ (плюс некоторое сглаживание)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"при наличии документа и слова в этом документе,\n",
    "    вернуть вес k-ой темы\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # удалить это слово / тематику из показателей,\n",
    "            # чтобы оно не влияло на веса\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # выбрать новую тематику на основе весов\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # и теперь снова увеличить показатели\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    document = get_document()\n",
    "\n",
    "    bigrams = list(zip(document, document[1:]))\n",
    "    transitions = defaultdict(list)\n",
    "    for prev, current in bigrams:\n",
    "        transitions[prev].append(current)\n",
    "\n",
    "    random.seed(0)\n",
    "    print(\"биграммные предложения\")\n",
    "    for i in range(10):\n",
    "        print(i, generate_using_bigrams(transitions))\n",
    "    print()\n",
    "\n",
    "    # trigrams\n",
    "\n",
    "    trigrams = list(zip(document, document[1:], document[2:]))\n",
    "    trigram_transitions = defaultdict(list)\n",
    "    starts = []\n",
    "\n",
    "    for prev, current, next in trigrams:\n",
    "\n",
    "        if prev == \".\":              # if the previous \"word\" was a period\n",
    "            starts.append(current)   # then this is a start word\n",
    "\n",
    "        trigram_transitions[(prev, current)].append(next)\n",
    "\n",
    "    print(\"триграммные предложения\")\n",
    "    for i in range(10):\n",
    "        print(i, generate_using_trigrams(starts, trigram_transitions))\n",
    "    print()\n",
    "\n",
    "    grammar = {\n",
    "        \"_S\"  : [\"_NP _VP\"],\n",
    "        \"_NP\" : [\"_N\",\n",
    "                 \"_A _NP _P _A _N\"],\n",
    "        \"_VP\" : [\"_V\",\n",
    "                 \"_V _NP\"],\n",
    "        \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "        \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "        \"_P\"  : [\"about\", \"near\"],\n",
    "        \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "    }\n",
    "\n",
    "    print(\"генерирование предложений на основе грамматики\")\n",
    "    for i in range(10):\n",
    "        print(i, \" \".join(generate_sentence(grammar)))\n",
    "    print()\n",
    "\n",
    "    print(\"сэмплирование по Гиббсу\")\n",
    "    comparison = compare_distributions()\n",
    "    for roll, (gibbs, direct) in comparison.items():\n",
    "        print(roll, gibbs, direct)\n",
    "\n",
    "\n",
    "    # тематическое моделирование\n",
    "\n",
    "    for k, word_counts in enumerate(topic_word_counts):\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count > 0: print(k, word, count)\n",
    "\n",
    "    topic_names = [\"Big Data and programming languages\",\n",
    "                   \"databases\",\n",
    "                   \"machine learning\",\n",
    "                   \"statistics\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print(document)\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print(topic_names[topic], count)\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
